{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.2MB 16kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-3.0.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "'''\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "'''\n",
    "\n",
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "dictionary.add_documents(texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a Dictionary from one or more text files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNow, how to read one-line-at-a-time from multiple files?\\n\\nclass ReadTxtFiles(object):\\n    def __init__(self, dirname):\\n        self.dirname = dirname\\n\\n    def __iter__(self):\\n        for fname in os.listdir(self.dirname):\\n            for line in open(os.path.join(self.dirname, fname), encoding=\\'latin\\'):\\n                yield simple_preprocess(line)\\n\\npath_to_text_directory = \"lsa_sports_food_docs\"\\n\\ndictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The advantage here is it let’s you read an entire text file without loading the file in memory all at once.\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('poem.txt', encoding='utf-8'))\n",
    "\n",
    "# Token to Id map\n",
    "#dictionary.token2id\n",
    "\n",
    "'''\n",
    "Now, how to read one-line-at-a-time from multiple files?\n",
    "\n",
    "class ReadTxtFiles(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='latin'):\n",
    "                yield simple_preprocess(line)\n",
    "\n",
    "path_to_text_directory = \"lsa_sports_food_docs\"\n",
    "\n",
    "dictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a bag of words corpus in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dogs': 0, 'let': 1, 'out': 2, 'the': 3, 'who': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a bag of words corpus from a text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "[(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n",
      "[(6, 1), (8, 1), (14, 1), (17, 2), (18, 1), (19, 1), (20, 1)]\n",
      "[(1, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]\n",
      "[]\n",
      "[(17, 2), (23, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(6, 1), (23, 1), (32, 1), (33, 1), (34, 1), (35, 1)]\n",
      "[(6, 1), (22, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]\n",
      "[(17, 1), (23, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]\n",
      "[(23, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1)]\n",
      "[]\n",
      "[(6, 1), (7, 1), (43, 1), (52, 1), (53, 1), (54, 1)]\n",
      "[(1, 1), (47, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1)]\n",
      "[(23, 1), (41, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1)]\n",
      "[(24, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 2), (70, 1)]\n",
      "[(71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1)]\n",
      "[]\n",
      "[(12, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1)]\n",
      "[(6, 1), (82, 2), (83, 1), (84, 1)]\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (6, 1), (85, 1)]\n",
      "[(14, 1), (23, 1), (31, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(6, 1), (23, 1), (43, 1), (89, 1), (90, 1), (91, 1), (92, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('poem.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to save a gensim dictionary and corpus to disk and load them back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0)]\n",
      "[(6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0)]\n",
      "[(6, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0)]\n",
      "[(6, 1.0), (8, 1.0), (14, 1.0), (17, 2.0), (18, 1.0), (19, 1.0), (20, 1.0)]\n",
      "[(1, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0)]\n",
      "[]\n",
      "[(17, 2.0), (23, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0)]\n",
      "[(6, 1.0), (23, 1.0), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0)]\n",
      "[(6, 1.0), (22, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 1.0), (40, 1.0)]\n",
      "[(17, 1.0), (23, 1.0), (41, 1.0), (42, 1.0), (43, 1.0), (44, 1.0), (45, 1.0)]\n",
      "[(23, 1.0), (46, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 1.0)]\n",
      "[]\n",
      "[(6, 1.0), (7, 1.0), (43, 1.0), (52, 1.0), (53, 1.0), (54, 1.0)]\n",
      "[(1, 1.0), (47, 1.0), (55, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 1.0)]\n",
      "[(23, 1.0), (41, 1.0), (60, 1.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0)]\n",
      "[(24, 1.0), (65, 1.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 2.0), (70, 1.0)]\n",
      "[(71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 1.0)]\n",
      "[]\n",
      "[(12, 1.0), (77, 1.0), (78, 1.0), (79, 1.0), (80, 1.0), (81, 1.0)]\n",
      "[(6, 1.0), (82, 2.0), (83, 1.0), (84, 1.0)]\n",
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (6, 1.0), (85, 1.0)]\n",
      "[(14, 1.0), (23, 1.0), (31, 1.0), (86, 1.0), (87, 1.0), (88, 1.0)]\n",
      "[(6, 1.0), (23, 1.0), (43, 1.0), (89, 1.0), (90, 1.0), (91, 1.0), (92, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Save the Dict and Corpus\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk\n",
    "\n",
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create the TFIDF matrix (corpus) in gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow is TFIDF computed?\\n\\nTf-Idf is computed by multiplying a local component like term frequency (TF) with a global component, that is, inverse document frequency (IDF) and optionally normalizing the result to unit length.\\n\\nAs a result of this, the words that occur frequently across documents will get downweighted.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How is TFIDF computed?\n",
    "\n",
    "Tf-Idf is computed by multiplying a local component like term frequency (TF) with a global component, that is, inverse document frequency (IDF) and optionally normalizing the result to unit length.\n",
    "\n",
    "As a result of this, the words that occur frequently across documents will get downweighted.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before TF-IDF\n",
      "[['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
      "[['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
      "[['this', 1], ['document', 1], ['third', 1]]\n",
      "after TF-IDF\n",
      "[['first', 0.63], ['is', 0.31], ['line', 0.63], ['the', 0.31], ['this', 0.13]]\n",
      "[['is', 0.31], ['the', 0.31], ['this', 0.13], ['second', 0.63], ['sentence', 0.63]]\n",
      "[['this', 0.14999999999999999], ['document', 0.69999999999999996], ['third', 0.69999999999999996]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "print(\"before TF-IDF\")\n",
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "print(\"after TF-IDF\")\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use gensim downloader API to load datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'file_name': 'glove-wiki-gigaword-50.gz',\n",
       " 'file_size': 69182535,\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'num_records': 400000,\n",
       " 'parameters': {'dimension': 50},\n",
       " 'parts': 1,\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Get information about the model or dataset\n",
    "api.info('glove-wiki-gigaword-50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create bigrams and trigrams using Phraser models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn paragraphs, certain words always tend to occur in pairs (bigram) or in groups of threes (trigram).\\nBecause the two words combined together form the actual entity. For example: The word ‘French’ refers the \\nlanguage or region and the word ‘revolution’ can refer to the planetary revolution.\\nBut combining them, ‘French Revolution’, refers to something completely different.'\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In paragraphs, certain words always tend to occur in pairs (bigram) or in groups of threes (trigram).\n",
    "Because the two words combined together form the actual entity. For example: The word ‘French’ refers the \n",
    "language or region and the word ‘revolution’ can refer to the planetary revolution.\n",
    "But combining them, ‘French Revolution’, refers to something completely different.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'n', ' ', 'p', 'a', 'r', 'a', 'g', 'r', 'a', 'p', 'h', 's', ',', ' ', 'c', 'e', 'r', 't', 'a', 'i', 'n', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'e', 'n', 'd', ' ', 't', 'o', ' ', 'o', 'c', 'c', 'u', 'r', ' ', 'i', 'n', ' ', 'p', 'a', 'i', 'r', 's', ' ', '(', 'b', 'i', 'g', 'r', 'a', 'm', ')', ' ', 'o', 'r', ' ', 'i', 'n', ' ', 'g', 'r', 'o', 'u', 'p', 's', ' ', 'o', 'f', ' ', 't', 'h', 'r', 'e', 'e', 's', ' ', '(', 't', 'r', 'i', 'g', 'r', 'a', 'm', ')', '.', '\\n', 'B', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', ' ', 't', 'w', 'o', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'e', 'd', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'r', 'm', ' ', 't', 'h', 'e', ' ', 'a', 'c', 't', 'u', 'a', 'l', ' ', 'e', 'n', 't', 'i', 't', 'y', '.', ' ', 'F', 'o', 'r', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ':', ' ', 'T', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ' ', '‘', 'F', 'r', 'e', 'n', 'c', 'h', '’', ' ', 'r', 'e', 'f', 'e', 'r', 's', ' ', 't', 'h', 'e', ' ', '\\n', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'o', 'r', ' ', 'r', 'e', 'g', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ' ', '‘', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '’', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'f', 'e', 'r', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'n', 'e', 't', 'a', 'r', 'y', ' ', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '.', '\\n', 'B', 'u', 't', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'm', ',', ' ', '‘', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'R', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '’', ',', ' ', 'r', 'e', 'f', 'e', 'r', 's', ' ', 't', 'o', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'l', 'y', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', '.']\n"
     ]
    }
   ],
   "source": [
    "dataset = [\"In paragraphs, certain words always tend to occur in pairs (bigram) or in groups of threes (trigram).\\nBecause the two words combined together form the actual entity. For example: The word ‘French’ refers the \\nlanguage or region and the word ‘revolution’ can refer to the planetary revolution.\\nBut combining them, ‘French Revolution’, refers to something completely different.\"]\n",
    "dataset = [wd for wd in dataset]\n",
    "dct = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)\n",
    "# Construct bigram\n",
    "print(bigram[dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'n', ' ', 'p', 'a', 'r', 'a', 'g', 'r', 'a', 'p', 'h', 's', ',', ' ', 'c', 'e', 'r', 't', 'a', 'i', 'n', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 't', 'e', 'n', 'd', ' ', 't', 'o', ' ', 'o', 'c', 'c', 'u', 'r', ' ', 'i', 'n', ' ', 'p', 'a', 'i', 'r', 's', ' ', '(', 'b', 'i', 'g', 'r', 'a', 'm', ')', ' ', 'o', 'r', ' ', 'i', 'n', ' ', 'g', 'r', 'o', 'u', 'p', 's', ' ', 'o', 'f', ' ', 't', 'h', 'r', 'e', 'e', 's', ' ', '(', 't', 'r', 'i', 'g', 'r', 'a', 'm', ')', '.', '\\n', 'B', 'e', 'c', 'a', 'u', 's', 'e', ' ', 't', 'h', 'e', ' ', 't', 'w', 'o', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'e', 'd', ' ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'r', 'm', ' ', 't', 'h', 'e', ' ', 'a', 'c', 't', 'u', 'a', 'l', ' ', 'e', 'n', 't', 'i', 't', 'y', '.', ' ', 'F', 'o', 'r', ' ', 'e', 'x', 'a', 'm', 'p', 'l', 'e', ':', ' ', 'T', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ' ', '‘', 'F', 'r', 'e', 'n', 'c', 'h', '’', ' ', 'r', 'e', 'f', 'e', 'r', 's', ' ', 't', 'h', 'e', ' ', '\\n', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'o', 'r', ' ', 'r', 'e', 'g', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ' ', '‘', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '’', ' ', 'c', 'a', 'n', ' ', 'r', 'e', 'f', 'e', 'r', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'n', 'e', 't', 'a', 'r', 'y', ' ', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '.', '\\n', 'B', 'u', 't', ' ', 'c', 'o', 'm', 'b', 'i', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'm', ',', ' ', '‘', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'R', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', '’', ',', ' ', 'r', 'e', 'f', 'e', 'r', 's', ' ', 't', 'o', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 't', 'e', 'l', 'y', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', '.']\n"
     ]
    }
   ],
   "source": [
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[dataset], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[dataset[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create Topic Models with LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Import packages and stopwords\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Import the dataset and get the text and real topic of each news article\n",
    "# Create the Dictionary\n",
    "data = [line for  line in open('poem.txt', encoding='utf-8')]\n",
    "#dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('poem.txt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pattern3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e5c51343396e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 2: Prepare Data (Remove stopwords and lemmatize)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpattern3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pattern3'"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare Data (Remove stopwords and lemmatize)\n",
    "import pattern3\n",
    "data_processed = []\n",
    "\n",
    "for i, doc in enumerate(data[:100]):\n",
    "    doc_out = []\n",
    "    for wd in doc:\n",
    "        if wd not in stop_words:  # remove stopwords\n",
    "            lemmatized_word = lemmatize(wd, allowed_tags=re.compile('(NN|JJ|RB)'))  # lemmatize\n",
    "            if lemmatized_word:\n",
    "                doc_out = doc_out + [lemmatized_word[0].split(b'/')[0].decode('utf-8')]\n",
    "        else:\n",
    "            continue\n",
    "    data_processed.append(doc_out)\n",
    "\n",
    "# Print a small sample    \n",
    "print(data_processed[0][:5]) \n",
    "\n",
    "# Step 3: Create the Inputs of LDA model: Dictionary and Corpus\n",
    "dct = corpora.Dictionary(data_processed)\n",
    "corpus = [dct.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 21:51:38,113 : INFO : using asymmetric alpha [0.26219156, 0.19027454, 0.14931786, 0.12287004, 0.10438152, 0.090729296, 0.080235206]\n",
      "2020-10-25 21:51:38,168 : INFO : using symmetric eta at 0.14285714285714285\n",
      "2020-10-25 21:51:38,168 : INFO : using serial LDA version on this node\n",
      "2020-10-25 21:51:38,171 : INFO : running online LDA training, 7 topics, 10 passes over the supplied corpus of 3 documents, updating every 31000 documents, evaluating every ~0 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2020-10-25 21:51:38,173 : INFO : training LDA model using 31 processes\n",
      "2020-10-25 21:51:38,487 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,494 : INFO : topic #6 (0.080): 0.119*\"sentence\" + 0.117*\"this\" + 0.116*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.108*\"document\" + 0.105*\"is\" + 0.100*\"first\"\n",
      "2020-10-25 21:51:38,495 : INFO : topic #5 (0.091): 0.129*\"second\" + 0.119*\"first\" + 0.111*\"is\" + 0.110*\"line\" + 0.110*\"third\" + 0.109*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.099*\"this\"\n",
      "2020-10-25 21:51:38,496 : INFO : topic #2 (0.149): 0.128*\"sentence\" + 0.127*\"line\" + 0.123*\"is\" + 0.109*\"this\" + 0.108*\"second\" + 0.107*\"first\" + 0.100*\"third\" + 0.099*\"the\" + 0.099*\"document\"\n",
      "2020-10-25 21:51:38,496 : INFO : topic #1 (0.190): 0.132*\"the\" + 0.130*\"this\" + 0.119*\"sentence\" + 0.112*\"is\" + 0.112*\"second\" + 0.103*\"first\" + 0.099*\"third\" + 0.096*\"line\" + 0.096*\"document\"\n",
      "2020-10-25 21:51:38,568 : INFO : topic #0 (0.262): 0.125*\"third\" + 0.123*\"document\" + 0.117*\"this\" + 0.115*\"line\" + 0.111*\"the\" + 0.109*\"is\" + 0.108*\"second\" + 0.102*\"sentence\" + 0.091*\"first\"\n",
      "2020-10-25 21:51:38,569 : INFO : topic diff=0.068218, rho=0.125000\n",
      "2020-10-25 21:51:38,570 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,573 : INFO : topic #6 (0.080): 0.119*\"sentence\" + 0.117*\"this\" + 0.116*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.108*\"document\" + 0.105*\"is\" + 0.100*\"first\"\n",
      "2020-10-25 21:51:38,574 : INFO : topic #5 (0.091): 0.129*\"second\" + 0.119*\"first\" + 0.111*\"is\" + 0.110*\"line\" + 0.110*\"third\" + 0.109*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.099*\"this\"\n",
      "2020-10-25 21:51:38,575 : INFO : topic #2 (0.149): 0.128*\"sentence\" + 0.127*\"line\" + 0.123*\"is\" + 0.109*\"this\" + 0.108*\"second\" + 0.107*\"first\" + 0.100*\"third\" + 0.099*\"the\" + 0.099*\"document\"\n",
      "2020-10-25 21:51:38,576 : INFO : topic #1 (0.190): 0.140*\"the\" + 0.138*\"this\" + 0.123*\"is\" + 0.116*\"sentence\" + 0.111*\"second\" + 0.103*\"first\" + 0.097*\"line\" + 0.088*\"third\" + 0.085*\"document\"\n",
      "2020-10-25 21:51:38,576 : INFO : topic #0 (0.262): 0.133*\"third\" + 0.131*\"document\" + 0.126*\"this\" + 0.110*\"line\" + 0.106*\"the\" + 0.105*\"is\" + 0.104*\"second\" + 0.098*\"sentence\" + 0.087*\"first\"\n",
      "2020-10-25 21:51:38,577 : INFO : topic diff=0.076824, rho=0.124032\n",
      "2020-10-25 21:51:38,578 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,581 : INFO : topic #6 (0.080): 0.118*\"sentence\" + 0.117*\"this\" + 0.116*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.108*\"document\" + 0.105*\"is\" + 0.100*\"first\"\n",
      "2020-10-25 21:51:38,582 : INFO : topic #5 (0.091): 0.128*\"second\" + 0.119*\"first\" + 0.111*\"is\" + 0.110*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.100*\"this\"\n",
      "2020-10-25 21:51:38,582 : INFO : topic #2 (0.149): 0.127*\"sentence\" + 0.126*\"line\" + 0.122*\"is\" + 0.109*\"this\" + 0.108*\"second\" + 0.108*\"first\" + 0.100*\"third\" + 0.099*\"the\" + 0.099*\"document\"\n",
      "2020-10-25 21:51:38,583 : INFO : topic #1 (0.190): 0.146*\"the\" + 0.145*\"this\" + 0.132*\"is\" + 0.114*\"sentence\" + 0.109*\"second\" + 0.103*\"first\" + 0.097*\"line\" + 0.078*\"third\" + 0.075*\"document\"\n",
      "2020-10-25 21:51:38,584 : INFO : topic #0 (0.262): 0.142*\"third\" + 0.140*\"document\" + 0.135*\"this\" + 0.105*\"line\" + 0.102*\"the\" + 0.100*\"is\" + 0.099*\"second\" + 0.094*\"sentence\" + 0.084*\"first\"\n",
      "2020-10-25 21:51:38,585 : INFO : topic diff=0.081878, rho=0.123089\n",
      "2020-10-25 21:51:38,585 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,668 : INFO : topic #6 (0.080): 0.118*\"sentence\" + 0.117*\"this\" + 0.116*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.108*\"document\" + 0.105*\"is\" + 0.100*\"first\"\n",
      "2020-10-25 21:51:38,669 : INFO : topic #5 (0.091): 0.128*\"second\" + 0.119*\"first\" + 0.111*\"is\" + 0.110*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.100*\"this\"\n",
      "2020-10-25 21:51:38,670 : INFO : topic #2 (0.149): 0.127*\"sentence\" + 0.126*\"line\" + 0.122*\"is\" + 0.110*\"this\" + 0.108*\"second\" + 0.108*\"first\" + 0.100*\"third\" + 0.100*\"the\" + 0.099*\"document\"\n",
      "2020-10-25 21:51:38,671 : INFO : topic #1 (0.190): 0.152*\"the\" + 0.151*\"this\" + 0.139*\"is\" + 0.113*\"sentence\" + 0.108*\"second\" + 0.103*\"first\" + 0.098*\"line\" + 0.069*\"third\" + 0.067*\"document\"\n",
      "2020-10-25 21:51:38,672 : INFO : topic #0 (0.262): 0.150*\"third\" + 0.148*\"document\" + 0.144*\"this\" + 0.101*\"line\" + 0.097*\"the\" + 0.096*\"is\" + 0.095*\"second\" + 0.090*\"sentence\" + 0.081*\"first\"\n",
      "2020-10-25 21:51:38,673 : INFO : topic diff=0.087532, rho=0.122167\n",
      "2020-10-25 21:51:38,673 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,676 : INFO : topic #6 (0.080): 0.118*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.105*\"is\" + 0.101*\"first\"\n",
      "2020-10-25 21:51:38,677 : INFO : topic #5 (0.091): 0.128*\"second\" + 0.118*\"first\" + 0.111*\"is\" + 0.110*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.100*\"this\"\n",
      "2020-10-25 21:51:38,678 : INFO : topic #2 (0.149): 0.127*\"sentence\" + 0.125*\"line\" + 0.122*\"is\" + 0.110*\"this\" + 0.108*\"second\" + 0.108*\"first\" + 0.101*\"third\" + 0.100*\"the\" + 0.100*\"document\"\n",
      "2020-10-25 21:51:38,678 : INFO : topic #1 (0.190): 0.157*\"the\" + 0.156*\"this\" + 0.146*\"is\" + 0.111*\"sentence\" + 0.107*\"second\" + 0.103*\"first\" + 0.098*\"line\" + 0.062*\"third\" + 0.060*\"document\"\n",
      "2020-10-25 21:51:38,679 : INFO : topic #0 (0.262): 0.158*\"third\" + 0.156*\"document\" + 0.152*\"this\" + 0.096*\"line\" + 0.093*\"the\" + 0.091*\"is\" + 0.091*\"second\" + 0.086*\"sentence\" + 0.077*\"first\"\n",
      "2020-10-25 21:51:38,680 : INFO : topic diff=0.093582, rho=0.121265\n",
      "2020-10-25 21:51:38,680 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,684 : INFO : topic #6 (0.080): 0.118*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.105*\"is\" + 0.101*\"first\"\n",
      "2020-10-25 21:51:38,685 : INFO : topic #5 (0.091): 0.127*\"second\" + 0.118*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.105*\"the\" + 0.101*\"this\"\n",
      "2020-10-25 21:51:38,685 : INFO : topic #2 (0.149): 0.126*\"sentence\" + 0.125*\"line\" + 0.122*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.101*\"third\" + 0.100*\"the\" + 0.100*\"document\"\n",
      "2020-10-25 21:51:38,769 : INFO : topic #1 (0.190): 0.161*\"the\" + 0.160*\"this\" + 0.151*\"is\" + 0.110*\"sentence\" + 0.107*\"second\" + 0.102*\"first\" + 0.099*\"line\" + 0.056*\"third\" + 0.054*\"document\"\n",
      "2020-10-25 21:51:38,769 : INFO : topic #0 (0.262): 0.166*\"third\" + 0.164*\"document\" + 0.161*\"this\" + 0.091*\"line\" + 0.088*\"the\" + 0.087*\"is\" + 0.086*\"second\" + 0.082*\"sentence\" + 0.074*\"first\"\n",
      "2020-10-25 21:51:38,770 : INFO : topic diff=0.099849, rho=0.120383\n",
      "2020-10-25 21:51:38,771 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,774 : INFO : topic #6 (0.080): 0.118*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.101*\"first\"\n",
      "2020-10-25 21:51:38,775 : INFO : topic #5 (0.091): 0.127*\"second\" + 0.118*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.106*\"the\" + 0.101*\"this\"\n",
      "2020-10-25 21:51:38,776 : INFO : topic #2 (0.149): 0.126*\"sentence\" + 0.125*\"line\" + 0.121*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.101*\"third\" + 0.101*\"the\" + 0.100*\"document\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 21:51:38,776 : INFO : topic #1 (0.190): 0.165*\"the\" + 0.164*\"this\" + 0.156*\"is\" + 0.109*\"sentence\" + 0.106*\"second\" + 0.102*\"first\" + 0.099*\"line\" + 0.050*\"third\" + 0.049*\"document\"\n",
      "2020-10-25 21:51:38,777 : INFO : topic #0 (0.262): 0.173*\"third\" + 0.172*\"document\" + 0.169*\"this\" + 0.087*\"line\" + 0.084*\"the\" + 0.083*\"is\" + 0.082*\"second\" + 0.078*\"sentence\" + 0.071*\"first\"\n",
      "2020-10-25 21:51:38,778 : INFO : topic diff=0.106160, rho=0.119520\n",
      "2020-10-25 21:51:38,778 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,782 : INFO : topic #6 (0.080): 0.117*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.102*\"first\"\n",
      "2020-10-25 21:51:38,783 : INFO : topic #5 (0.091): 0.126*\"second\" + 0.118*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.107*\"sentence\" + 0.106*\"the\" + 0.101*\"this\"\n",
      "2020-10-25 21:51:38,783 : INFO : topic #2 (0.149): 0.125*\"sentence\" + 0.124*\"line\" + 0.121*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.102*\"third\" + 0.101*\"the\" + 0.101*\"document\"\n",
      "2020-10-25 21:51:38,784 : INFO : topic #1 (0.190): 0.168*\"the\" + 0.167*\"this\" + 0.160*\"is\" + 0.108*\"sentence\" + 0.105*\"second\" + 0.102*\"first\" + 0.099*\"line\" + 0.046*\"third\" + 0.045*\"document\"\n",
      "2020-10-25 21:51:38,785 : INFO : topic #0 (0.262): 0.181*\"third\" + 0.179*\"document\" + 0.177*\"this\" + 0.083*\"line\" + 0.080*\"the\" + 0.079*\"is\" + 0.079*\"second\" + 0.075*\"sentence\" + 0.068*\"first\"\n",
      "2020-10-25 21:51:38,785 : INFO : topic diff=0.112345, rho=0.118676\n",
      "2020-10-25 21:51:38,786 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,869 : INFO : topic #6 (0.080): 0.117*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.102*\"first\"\n",
      "2020-10-25 21:51:38,870 : INFO : topic #5 (0.091): 0.125*\"second\" + 0.117*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.108*\"sentence\" + 0.106*\"the\" + 0.102*\"this\"\n",
      "2020-10-25 21:51:38,871 : INFO : topic #2 (0.149): 0.125*\"sentence\" + 0.124*\"line\" + 0.121*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.102*\"third\" + 0.101*\"the\" + 0.101*\"document\"\n",
      "2020-10-25 21:51:38,871 : INFO : topic #1 (0.190): 0.170*\"the\" + 0.170*\"this\" + 0.164*\"is\" + 0.107*\"sentence\" + 0.105*\"second\" + 0.102*\"first\" + 0.099*\"line\" + 0.042*\"third\" + 0.041*\"document\"\n",
      "2020-10-25 21:51:38,872 : INFO : topic #0 (0.262): 0.188*\"third\" + 0.186*\"document\" + 0.184*\"this\" + 0.079*\"line\" + 0.076*\"the\" + 0.076*\"is\" + 0.075*\"second\" + 0.071*\"sentence\" + 0.065*\"first\"\n",
      "2020-10-25 21:51:38,873 : INFO : topic diff=0.118235, rho=0.117849\n",
      "2020-10-25 21:51:38,873 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #3/3, outstanding queue size 1\n",
      "2020-10-25 21:51:38,877 : INFO : topic #6 (0.080): 0.117*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.102*\"first\"\n",
      "2020-10-25 21:51:38,877 : INFO : topic #5 (0.091): 0.125*\"second\" + 0.117*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.108*\"sentence\" + 0.106*\"the\" + 0.102*\"this\"\n",
      "2020-10-25 21:51:38,878 : INFO : topic #2 (0.149): 0.124*\"sentence\" + 0.123*\"line\" + 0.120*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.102*\"third\" + 0.102*\"the\" + 0.102*\"document\"\n",
      "2020-10-25 21:51:38,879 : INFO : topic #1 (0.190): 0.173*\"the\" + 0.172*\"this\" + 0.167*\"is\" + 0.106*\"sentence\" + 0.104*\"second\" + 0.102*\"first\" + 0.100*\"line\" + 0.038*\"third\" + 0.037*\"document\"\n",
      "2020-10-25 21:51:38,880 : INFO : topic #0 (0.262): 0.194*\"third\" + 0.193*\"document\" + 0.191*\"this\" + 0.075*\"line\" + 0.073*\"the\" + 0.072*\"is\" + 0.071*\"second\" + 0.068*\"sentence\" + 0.063*\"first\"\n",
      "2020-10-25 21:51:38,880 : INFO : topic diff=0.123665, rho=0.117039\n",
      "2020-10-25 21:51:39,270 : INFO : saving LdaState object under lda_model.model.state, separately None\n",
      "2020-10-25 21:51:39,296 : INFO : saved lda_model.model.state\n",
      "2020-10-25 21:51:39,319 : INFO : saving LdaMulticore object under lda_model.model, separately ['expElogbeta', 'sstats']\n",
      "2020-10-25 21:51:39,320 : INFO : storing np array 'expElogbeta' to lda_model.model.expElogbeta.npy\n",
      "2020-10-25 21:51:39,374 : INFO : not storing attribute dispatcher\n",
      "2020-10-25 21:51:39,375 : INFO : not storing attribute state\n",
      "2020-10-25 21:51:39,375 : INFO : not storing attribute id2word\n",
      "2020-10-25 21:51:39,411 : INFO : saved lda_model.model\n",
      "2020-10-25 21:51:39,412 : INFO : topic #0 (0.262): 0.194*\"third\" + 0.193*\"document\" + 0.191*\"this\" + 0.075*\"line\" + 0.073*\"the\" + 0.072*\"is\" + 0.071*\"second\" + 0.068*\"sentence\" + 0.063*\"first\"\n",
      "2020-10-25 21:51:39,413 : INFO : topic #1 (0.190): 0.173*\"the\" + 0.172*\"this\" + 0.167*\"is\" + 0.106*\"sentence\" + 0.104*\"second\" + 0.102*\"first\" + 0.100*\"line\" + 0.038*\"third\" + 0.037*\"document\"\n",
      "2020-10-25 21:51:39,413 : INFO : topic #2 (0.149): 0.124*\"sentence\" + 0.123*\"line\" + 0.120*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.102*\"third\" + 0.102*\"the\" + 0.102*\"document\"\n",
      "2020-10-25 21:51:39,414 : INFO : topic #3 (0.123): 0.123*\"first\" + 0.115*\"this\" + 0.113*\"document\" + 0.112*\"the\" + 0.111*\"third\" + 0.111*\"second\" + 0.105*\"line\" + 0.105*\"sentence\" + 0.105*\"is\"\n",
      "2020-10-25 21:51:39,415 : INFO : topic #4 (0.104): 0.129*\"this\" + 0.119*\"is\" + 0.118*\"first\" + 0.116*\"the\" + 0.113*\"sentence\" + 0.105*\"second\" + 0.101*\"document\" + 0.101*\"line\" + 0.098*\"third\"\n",
      "2020-10-25 21:51:39,415 : INFO : topic #5 (0.091): 0.125*\"second\" + 0.117*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.108*\"sentence\" + 0.106*\"the\" + 0.102*\"this\"\n",
      "2020-10-25 21:51:39,416 : INFO : topic #6 (0.080): 0.117*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.102*\"first\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.194*\"third\" + 0.193*\"document\" + 0.191*\"this\" + 0.075*\"line\" + 0.073*\"the\" + 0.072*\"is\" + 0.071*\"second\" + 0.068*\"sentence\" + 0.063*\"first\"'),\n",
       " (1,\n",
       "  '0.173*\"the\" + 0.172*\"this\" + 0.167*\"is\" + 0.106*\"sentence\" + 0.104*\"second\" + 0.102*\"first\" + 0.100*\"line\" + 0.038*\"third\" + 0.037*\"document\"'),\n",
       " (2,\n",
       "  '0.124*\"sentence\" + 0.123*\"line\" + 0.120*\"is\" + 0.110*\"this\" + 0.109*\"second\" + 0.108*\"first\" + 0.102*\"third\" + 0.102*\"the\" + 0.102*\"document\"'),\n",
       " (3,\n",
       "  '0.123*\"first\" + 0.115*\"this\" + 0.113*\"document\" + 0.112*\"the\" + 0.111*\"third\" + 0.111*\"second\" + 0.105*\"line\" + 0.105*\"sentence\" + 0.105*\"is\"'),\n",
       " (4,\n",
       "  '0.129*\"this\" + 0.119*\"is\" + 0.118*\"first\" + 0.116*\"the\" + 0.113*\"sentence\" + 0.105*\"second\" + 0.101*\"document\" + 0.101*\"line\" + 0.098*\"third\"'),\n",
       " (5,\n",
       "  '0.125*\"second\" + 0.117*\"first\" + 0.111*\"is\" + 0.111*\"line\" + 0.110*\"third\" + 0.110*\"document\" + 0.108*\"sentence\" + 0.106*\"the\" + 0.102*\"this\"'),\n",
       " (6,\n",
       "  '0.117*\"sentence\" + 0.116*\"this\" + 0.115*\"second\" + 0.113*\"line\" + 0.111*\"third\" + 0.111*\"the\" + 0.109*\"document\" + 0.106*\"is\" + 0.102*\"first\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=7,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "lda_model.save('lda_model.model')\n",
    "\n",
    "# See the topics\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train Word2Vec model using gensim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 21:51:45,069 : INFO : collecting all words and their counts\n",
      "2020-10-25 21:51:45,070 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2020-10-25 21:51:45,071 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-10-25 21:51:45,071 : INFO : collected 40 word types from a corpus of 729 raw words and 23 sentences\n",
      "2020-10-25 21:51:45,072 : INFO : Loading a fresh vocabulary\n",
      "2020-10-25 21:51:45,072 : INFO : effective_min_count=0 retains 40 unique words (100% of original 40, drops 0)\n",
      "2020-10-25 21:51:45,073 : INFO : effective_min_count=0 leaves 729 word corpus (100% of original 729, drops 0)\n",
      "2020-10-25 21:51:45,074 : INFO : deleting the raw counts dictionary of 40 items\n",
      "2020-10-25 21:51:45,074 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2020-10-25 21:51:45,075 : INFO : downsampling leaves estimated 144 word corpus (19.8% of prior 729)\n",
      "2020-10-25 21:51:45,075 : INFO : estimated required memory for 40 words and 100 dimensions: 52000 bytes\n",
      "2020-10-25 21:51:45,076 : INFO : resetting layer weights\n",
      "2020-10-25 21:51:45,077 : INFO : training model with 32 workers on 40 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-10-25 21:51:45,086 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2020-10-25 21:51:45,086 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2020-10-25 21:51:45,087 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2020-10-25 21:51:45,087 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2020-10-25 21:51:45,088 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2020-10-25 21:51:45,088 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2020-10-25 21:51:45,089 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2020-10-25 21:51:45,089 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2020-10-25 21:51:45,090 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2020-10-25 21:51:45,090 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2020-10-25 21:51:45,091 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2020-10-25 21:51:45,168 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2020-10-25 21:51:45,169 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2020-10-25 21:51:45,169 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2020-10-25 21:51:45,170 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2020-10-25 21:51:45,170 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2020-10-25 21:51:45,171 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-10-25 21:51:45,171 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-10-25 21:51:45,172 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-10-25 21:51:45,172 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-10-25 21:51:45,173 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-10-25 21:51:45,173 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-10-25 21:51:45,174 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-10-25 21:51:45,174 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-10-25 21:51:45,175 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-10-25 21:51:45,175 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-10-25 21:51:45,176 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 21:51:45,176 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 21:51:45,177 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 21:51:45,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 21:51:45,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 21:51:45,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 21:51:45,179 : INFO : EPOCH - 1 : training on 729 raw words (142 effective words) took 0.1s, 1496 effective words/s\n",
      "2020-10-25 21:51:45,186 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2020-10-25 21:51:45,187 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2020-10-25 21:51:45,187 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2020-10-25 21:51:45,188 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2020-10-25 21:51:45,188 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2020-10-25 21:51:45,268 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2020-10-25 21:51:45,269 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2020-10-25 21:51:45,269 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2020-10-25 21:51:45,270 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2020-10-25 21:51:45,270 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2020-10-25 21:51:45,271 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2020-10-25 21:51:45,271 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2020-10-25 21:51:45,272 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2020-10-25 21:51:45,273 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2020-10-25 21:51:45,273 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2020-10-25 21:51:45,274 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2020-10-25 21:51:45,274 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-10-25 21:51:45,275 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-10-25 21:51:45,275 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-10-25 21:51:45,276 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-10-25 21:51:45,277 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-10-25 21:51:45,277 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-10-25 21:51:45,278 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-10-25 21:51:45,278 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-10-25 21:51:45,279 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-10-25 21:51:45,280 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-10-25 21:51:45,280 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 21:51:45,281 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 21:51:45,281 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 21:51:45,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 21:51:45,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 21:51:45,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 21:51:45,283 : INFO : EPOCH - 2 : training on 729 raw words (155 effective words) took 0.1s, 1567 effective words/s\n",
      "2020-10-25 21:51:45,295 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2020-10-25 21:51:45,369 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2020-10-25 21:51:45,369 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2020-10-25 21:51:45,370 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2020-10-25 21:51:45,370 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2020-10-25 21:51:45,371 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2020-10-25 21:51:45,372 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2020-10-25 21:51:45,372 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2020-10-25 21:51:45,373 : INFO : worker thread finished; awaiting finish of 23 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 21:51:45,373 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2020-10-25 21:51:45,374 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2020-10-25 21:51:45,374 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2020-10-25 21:51:45,375 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2020-10-25 21:51:45,375 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2020-10-25 21:51:45,376 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2020-10-25 21:51:45,376 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2020-10-25 21:51:45,377 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-10-25 21:51:45,377 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-10-25 21:51:45,378 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-10-25 21:51:45,378 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-10-25 21:51:45,379 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-10-25 21:51:45,379 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-10-25 21:51:45,380 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-10-25 21:51:45,380 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-10-25 21:51:45,381 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-10-25 21:51:45,381 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-10-25 21:51:45,382 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 21:51:45,382 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 21:51:45,383 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 21:51:45,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 21:51:45,384 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 21:51:45,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 21:51:45,385 : INFO : EPOCH - 3 : training on 729 raw words (142 effective words) took 0.1s, 1497 effective words/s\n",
      "2020-10-25 21:51:45,472 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2020-10-25 21:51:45,473 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2020-10-25 21:51:45,474 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2020-10-25 21:51:45,474 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2020-10-25 21:51:45,475 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2020-10-25 21:51:45,475 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2020-10-25 21:51:45,476 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2020-10-25 21:51:45,476 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2020-10-25 21:51:45,477 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2020-10-25 21:51:45,477 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2020-10-25 21:51:45,478 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2020-10-25 21:51:45,478 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2020-10-25 21:51:45,479 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2020-10-25 21:51:45,479 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2020-10-25 21:51:45,480 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2020-10-25 21:51:45,481 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2020-10-25 21:51:45,481 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-10-25 21:51:45,482 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-10-25 21:51:45,482 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-10-25 21:51:45,483 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-10-25 21:51:45,483 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-10-25 21:51:45,484 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-10-25 21:51:45,484 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-10-25 21:51:45,485 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-10-25 21:51:45,485 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-10-25 21:51:45,486 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-10-25 21:51:45,486 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 21:51:45,487 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 21:51:45,487 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 21:51:45,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 21:51:45,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 21:51:45,568 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-10-25 21:51:45,569 : INFO : EPOCH - 4 : training on 729 raw words (136 effective words) took 0.1s, 1378 effective words/s\n",
      "2020-10-25 21:51:45,577 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2020-10-25 21:51:45,578 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2020-10-25 21:51:45,579 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2020-10-25 21:51:45,579 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2020-10-25 21:51:45,580 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2020-10-25 21:51:45,580 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2020-10-25 21:51:45,581 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2020-10-25 21:51:45,581 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2020-10-25 21:51:45,582 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2020-10-25 21:51:45,582 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2020-10-25 21:51:45,583 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2020-10-25 21:51:45,583 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2020-10-25 21:51:45,584 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2020-10-25 21:51:45,584 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2020-10-25 21:51:45,585 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2020-10-25 21:51:45,585 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2020-10-25 21:51:45,586 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2020-10-25 21:51:45,586 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2020-10-25 21:51:45,587 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2020-10-25 21:51:45,587 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2020-10-25 21:51:45,588 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-10-25 21:51:45,588 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-10-25 21:51:45,668 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-10-25 21:51:45,669 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-10-25 21:51:45,669 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-10-25 21:51:45,670 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-10-25 21:51:45,670 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-10-25 21:51:45,671 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-10-25 21:51:45,671 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-10-25 21:51:45,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-10-25 21:51:45,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-10-25 21:51:45,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-25 21:51:45,673 : INFO : EPOCH - 5 : training on 729 raw words (150 effective words) took 0.1s, 1535 effective words/s\n",
      "2020-10-25 21:51:45,674 : INFO : training on a 3645 raw words (725 effective words) took 0.6s, 1216 effective words/s\n",
      "2020-10-25 21:51:45,674 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'Two' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-135a81a94cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get the word vector for given word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Two'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 )\n\u001b[0;32m-> 1461\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Two' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "data = [line for  line in open('poem.txt', encoding='utf-8')]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Get the word vector for given word\n",
    "model['Two']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two roads diverged in a yellow wood,\\n',\n",
       " 'And sorry I could not travel both\\n',\n",
       " 'And be one traveler, long I stood\\n',\n",
       " 'And looked down one as far as I could\\n',\n",
       " 'To where it bent in the undergrowth;\\n',\n",
       " '\\n',\n",
       " 'Then took the other, as just as fair,\\n',\n",
       " 'And having perhaps the better claim,\\n',\n",
       " 'Because it was grassy and wanted wear;\\n',\n",
       " 'Though as for that the passing there\\n',\n",
       " 'Had worn them really about the same,\\n',\n",
       " '\\n',\n",
       " 'And both that morning equally lay\\n',\n",
       " 'In leaves no step had trodden black.\\n',\n",
       " 'Oh, I kept the first for another day!\\n',\n",
       " 'Yet knowing how way leads on to way,\\n',\n",
       " 'I doubted if I should ever come back.\\n',\n",
       " '\\n',\n",
       " 'I shall be telling this with a sigh\\n',\n",
       " 'Somewhere ages and ages hence:\\n',\n",
       " 'Two roads diverged in a wood, and I—\\n',\n",
       " 'I took the one less traveled by,\\n',\n",
       " 'And that has made all the difference.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
